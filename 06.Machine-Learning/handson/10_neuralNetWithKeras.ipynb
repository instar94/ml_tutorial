{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05787215",
   "metadata": {},
   "source": [
    "# 인공 신경망 소개 with 케라스 \n",
    "\n",
    "> 인공신경망은 뇌에 있는 생물학적 뉴런 네트워크에서 영감을 받은 머신러닝 모델 \n",
    "\n",
    "- 딥러닝의 핵심\n",
    "\n",
    "## 주요 내용 \n",
    "- 인공신경망의 초장기 구조 소개 \n",
    "- `다층 퍼셉트론` 구조 소개 \n",
    "- `케라스`를 이용한 인공 신경망 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a8aed",
   "metadata": {},
   "source": [
    "# 1. 생물학적 뉴런에서 인공 뉴런까지 \n",
    "## 생물학적 뉴런 \n",
    "- `수상돌기` : 신호를 받아들임\n",
    "- `축삭돌기` : 신경 전달 물질을 생성\n",
    "- `시냅스` : 신경 전달 물질을 전달\n",
    "\n",
    "## 뉴런을 사용한 논리 연산 \n",
    "- `인공 뉴런` : 하나 이상의 이진 입력과 출력을 가짐\n",
    "- 단순히 입력이 일정 개수 만큼 활성화 되었을 때 출력\n",
    "- 이러한 구조를 통해 어떤 명제도 계산 가능\n",
    "\n",
    "## 퍼셉트론 \n",
    "### 정의\n",
    "> 퍼셉트론이란 가장 간단한 인공 신경망 구조\n",
    "- `TLU` 혹은 `LTU` 라고도 불림\n",
    "\n",
    "### 원리 \n",
    "- 입력과 출력이 이진값이 아닌 `숫자`이고, 각 입력 연결은 가중치와 연관됨 \n",
    "\n",
    "#### 과정\n",
    "1) 입력 선형 함수 계산 \n",
    "  - 입력 선형 함수 = $z = w_1x_1 + w_2x_2 + ... w_nx_n + b = w^Tx+b $\n",
    "2) `계단 함수` 계산 \n",
    "  - 입력 선형 함수의 결과를 계단 함수 $h_w(x) = step(z)$에 적용 \n",
    "  - `계단 함수` : 그래프를 그렸을 때, 계단처럼 층이 나눠진 함수 \n",
    "  - 주로 `헤비사이드 계산함수` 혹은 `부호 함수`를 사용\n",
    "\n",
    "#### 헤비사이드 계단 함수 \n",
    "$$\n",
    "\\textrm{heaviside(z)} = \\begin{cases}\n",
    "   0 &\\text{if } z<0 \\\\\n",
    "   1 &\\text{if } z \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "\n",
    "#### 부호 함수 \n",
    "$$\n",
    "\\textrm{sgn(z)} = \\begin{cases}\n",
    "   -1 &\\text{if } z<0 \\\\\n",
    "   0 &\\text{if } z=0 \\\\\n",
    "   +1 &\\text{if } z> 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### 퍼셉트론의 구조 \n",
    "- `밀집층` : TLU가 모든 입력에 연결되어 있는 층\n",
    "- `입력층` : 입력을 구성\n",
    "- `출력층` : 최종 출력을 생성하는 층 \n",
    "\n",
    "### 출력층의 계산 \n",
    "$$\n",
    "h_{W,b}(X) = \\phi(XW + b)\n",
    "$$\n",
    "\n",
    "- $X$ : 입력 특성의 행렬\n",
    "  - 행 : 샘플 \n",
    "  - 열 : 특성 \n",
    "- $W$ : 가중치 행렬 \n",
    "  - 모든 연결 가중치 포함 \n",
    "  - 행 : 입력 \n",
    "  - 열 : 출력층의 뉴런 \n",
    "- $b$ : 편향 벡터 \n",
    "  - 모든 뉴런의 편향을 포함\n",
    "- $\\phi$ : 활성화 함수 \n",
    "  - TLU의 경우 계단함수 \n",
    "\n",
    "### 퍼셉트론의 훈련 \n",
    "> 퍼셉트론은 `헤브의 규칙` 알고리즘을 이용하여 학습 \n",
    "- 뉴런이 다른 뉴런을 활성화 시킬 때 이 두 뉴런의 연결이 강해진다고 제안 \n",
    "- 즉, 뉴런이 `동시에 활성화` 될 때 마다 이들 사이의 `연결 가중치`가  증가하는 경향 \n",
    "\n",
    "#### 원리 \n",
    "- 퍼셉트론은 네트워크가 예측 할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련\n",
    "- `오차`가 `감소`되도록 연결을 강화 \n",
    "  - 한 개의 샘플이 주입되면 각 샘플에 대한 예측이 만들어짐 \n",
    "  - `잘못된 예측` -> 입력 가중치 강화 \n",
    "\n",
    "#### 퍼셉트론의 가중치 업데이트 규칙\n",
    "$$ \n",
    "w_{i,j}^{(\\textrm{next step})} = w_{i,j} + \\eta \\cdot (y_j - \\hat{y}_j) \\cdot x_i\n",
    "$$\n",
    "\n",
    "|기호|의미|\n",
    "|---|---|\n",
    "|$w_{i,j}$ | i번째 입력 뉴런과 j 번째 출력 뉴런 사이 `연결 가중치`|\n",
    "|$x_i$ | 현재 훈련 샘플의 i번째 뉴런의 `입력값`|\n",
    "|$\\hat{y}_j$ | 현재 훈련 샘플의 j번째 출력 뉴런의 `출력값`|\n",
    "|$y_j$ | 현재 훈련 샘플의 j번째 뉴런의 `타깃값`|\n",
    "|$\\eta$ | 학습률|\n",
    "\n",
    "#### 퍼셉트론 수렴 이론 \n",
    "- 출력 뉴런의 결정 경계는 `선형`\n",
    "  - 복잡한 패턴의 경우 학습 불가 \n",
    "- 훈련 샘플이 선형적으로 구분 될 수 있다면 정답에 수렴할 수 있음 \n",
    "\n",
    "\n",
    "### 사이킷런에서 퍼셉트론 구현하기 \n",
    "- 클래스 : Perceptron\n",
    "  - SGD로 구현 가능 \n",
    "  - loss = \"perceptron\"\n",
    "  - learning_rate=\"constant\"\n",
    "  - eat0=1\n",
    "  - penalty=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384eead5",
   "metadata": {},
   "source": [
    "- 예시 : 붓꽃 데이터를 이용한 퍼셉트론 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3745c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확인 :  [ True False]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "### 붓꽃 데이터 로드 \n",
    "iris = load_iris(as_frame=True)\n",
    "X=iris.data[[\"petal length (cm)\",\"petal width (cm)\"]].values\n",
    "\n",
    "### 종이 Iris-setosa 인 것만\n",
    "y=(iris.target==0)\n",
    "\n",
    "### 모델 학습\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "###예측 시행 \n",
    "X_new = [[2,0.5],[3,1]]\n",
    "y_pred = per_clf.predict(X_new)\n",
    "print(\"예측 확인 : \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4dca9",
   "metadata": {},
   "source": [
    "## 다중 퍼셉트론(MLP)\n",
    "> 퍼셉트론을 여러 개 쌓아올려 일부의 제약을 줄임 \n",
    "- 퍼셉트론에서 사용 할 수 없었던 `XOR`을 `다중 퍼셉트론`에서는 해결 가능 \n",
    "\n",
    "### 구성 \n",
    "- 입력층 \n",
    "  - 입력층과 가까운 층 = `하위층`\n",
    "- 출력층 \n",
    "  - 출력층과 가까운 층 = `상위층`\n",
    "- `은닉층`\n",
    "  - 은닉층을 여러 개 쌓아올린 신경망 = `심층신경망(DNN)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
